    def backprop(self, x, y):
        """"""
        Возвращает кортеж ``(nabla_b, nabla_w)`` -- градиент целевой функции по всем параметрам сети.
        ``nabla_b`` и ``nabla_w`` -- послойные списки массивов ndarray,
        такие же, как self.biases и self.weights соответственно.
        """"""
        # Эту функцию необходимо реализовать - будет сделано.
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        # прямое распространение (forward pass)

        z = [] # список векторов суммаций всех слоев
        a = [] # список векторов вых. активаций всех слоев
        z.append(None) # у нейронов 0-го слоя (""рецепторов"") 
                       # нету сумматорной функции
        a.append(x)    # x - активация нулевого слоя (""рецепторов"")
                       # a[0] = x - входная активация
        for b, w in zip(self.biases, self.weights):
            z.append(np.dot(w, a[-1]) + b)
            # подсчитать сумматорную ф-ю следующего слоя
            # на основании последнего подсчитанного a[-1]
            # и добавить ее в список векторов суммаций
            # (ее индекс будет соответствовать номеру слоя)
            a.append(sigmoid(z[-1]))
            # подсчитать активационную ф-ю следующего слоя
            # на основании последнего подсчитанного z[-1]
            # и добавить ее в список векторов активаций
            # (ее индекс будет соответствовать номеру слоя)
                     
        # обратное распространение (backward pass)
        
        delta = (a[-1] - y) * sigmoid_prime(z[-1]) # ошибка выходного слоя
        nabla_b[-1] = delta              # производная J по смещениям выходного слоя
        nabla_w[-1] = delta.dot(a[-2].T) # производная J по весам выходного слоя
                     # размерность nabla_w[-1] равна j x k,
                     # как и в соответствующей матрицы весов из списка self.weights
                     # (то есть self.weights[-1])
                     # где j - количество нейронов выходного слоя
                     #     k - количество нейронов предпоследнего слоя
                     # тогда каждый элемент nabla_w[-1] - это произведение 
                     # delta^L_j * a^l-1_k

        # Обратите внимание, что переменная l в цикле ниже используется
        # немного иначе, чем в лекциях.  Здесь l = 1 означает последний слой, 
        # l = 2 - предпоследний и так далее.  
        # Мы перенумеровали схему, чтобы с удобством для себя 
        # использовать тот факт, что в Python к переменной типа list 
        # можно обращаться по негативному индексу.
        for l in range(2, self.num_layers):
            # дополнительные вычисления, чтобы легче записывалось
            #
            delta = np.dot(self.weights[-l+1].T, delta) * sigmoid_prime(z[-l]) # ошибка на слое L-l
            nabla_b[-l] = delta # производная J по смещениям L-l-го слоя
            nabla_w[-l] = delta.dot(a[-l-1].T) # производная J по весам L-l-го слоя
        return nabla_b, nabla_w 